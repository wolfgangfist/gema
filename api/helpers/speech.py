
import torch

# --------------- Check CUDA ---------------
if not torch.cuda.is_available():
    print("âŒ CUDA is not available. This API requires a GPU.")
    raise RuntimeError("CUDA is not available. This API requires a GPU.")

# --------------- Load model ---------------
print("ğŸš€ Loading CSM model onto GPU...")
#generator = load_csm_1b(device="cuda")
print("âœ… Model loaded.")